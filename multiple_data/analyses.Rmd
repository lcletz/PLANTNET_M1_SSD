---
title: "Mutliple_data_analyses"
author: "AIGOIN Emilie"
date: "2025-02-16"
output: html_document
---

```{r}

# Charger les bibliothèques nécessaires
library(jsonlite)
library(dplyr)
library(purrr)

```

```{r}

# Fonction pour traiter un fichier JSON individuel
process_single_json <- function(file_path, id_converter) {
  # Lire le fichier JSON
  json_data <- tryCatch(
    fromJSON(file_path),
    error = function(e) {
      warning(paste("Erreur lors de la lecture du fichier:", file_path))
      return(NULL)
    }
  )
  
  if (is.null(json_data)) return(NULL)
  
  # Extraire l'ID de l'observation depuis le nom du fichier
  obs_id <- tools::file_path_sans_ext(basename(file_path))
  
  # Extraire les prédictions et scores
  results <- json_data$results
  
  # Créer un dictionnaire des scores par ID PL-CrowdSE
  predictions <- results$score
  names(predictions) <- id_converter[results$id]
  
  # Retourner uniquement les prédictions valides (où l'ID existe dans converter)
  predictions <- predictions[!is.na(names(predictions))]
  
  if (length(predictions) == 0) return(NULL)
  
  return(list(
    obs_id = obs_id,
    predictions = predictions
  ))
}

```

```{r}

# Fonction principale
process_plant_data <- function(base_dir = "00", 
                               converter_path = "tasks.json", 
                               output_path = "output_consolidated.json", 
                               max_files = 5) {
  
  # Charger le fichier de conversion des IDs
  id_converter <- fromJSON(converter_path)
  
  # Lister tous les sous-dossiers dans le dossier 00
  subdirs <- list.files(base_dir, full.names = TRUE)
  
  # Initialiser une liste pour stocker tous les résultats
  all_results <- list()
  files_processed <- 0  # Compteur pour limiter le nombre total de fichiers traités
  
  # Pour chaque sous-dossier
  for (subdir in subdirs) {
    # Lister tous les fichiers JSON dans le sous-dossier
    json_files <- list.files(subdir, 
                            pattern = "\\.json$", 
                            full.names = TRUE)
    
    # Traiter chaque fichier JSON
    results <- map(json_files, ~process_single_json(.x, id_converter))
    
    # Filtrer les résultats NULL
    results <- results[!sapply(results, is.null)]
    
    # Ajouter à nos résultats globaux
    all_results <- c(all_results, results)
    
    # Mettre à jour le compteur
    files_processed <- files_processed + length(json_files)
    
    # Afficher la progression
    cat(sprintf("Traitement du dossier %s terminé\n", basename(subdir)))
  }
  
  # Arrêter si on a atteint le nombre maximal de fichiers à traiter
    if (files_processed >= max_files) break
  }
  
  # Créer la structure finale
  final_results <- setNames(
    map(all_results, ~.x$predictions),
    map_chr(all_results, ~.x$obs_id)
  )
  
  # Sauvegarder le résultat
  write_json(final_results, output_path, auto_unbox = TRUE)
  
  return(final_results)
}

```

```{r}

# Pour tester avec votre structure :
results <- process_plant_data(
  base_dir = "00",              # Le dossier principal contenant les sous-dossiers
  converter_path = "tasks.json", # Le fichier de conversion des IDs
  output_path = "output_consolidated.json",
  max_files = 5
)

```

```{r}

# Afficher quelques statistiques sur les résultats
cat("Nombre total d'observations traitées:", length(results), "\n")
cat("Exemple de la première observation:\n")
print(results[[1]])

```

Test d'une autre méthode 

```{r}

library(jsonlite)
library(dplyr)
library(purrr)
library(furrr)
library(progressr)
library(future)

# Configuration du traitement parallèle
plan(multisession)

# La fonction process_single_json reste la même que précédemment

# Fonction principale modifiée pour ne traiter que les deux premiers dossiers
process_plant_data <- function(base_dir = "00", 
                             converter_path = "tasks.json", 
                             output_path = "output_test.json",  # nom différent pour le test
                             batch_size = 1000) {
  # Charger le fichier de conversion des IDs
  id_converter <- fromJSON(converter_path)
  
  # Lister les sous-dossiers et ne prendre que les deux premiers
  subdirs <- list.files(base_dir, full.names = TRUE)[1]
  
  handlers(global = TRUE)
  with_progress({
    p <- progressor(along = NULL)
    
    # Obtenir la liste des fichiers uniquement dans les deux premiers sous-dossiers
    all_files <- unlist(lapply(subdirs, function(d) {
      list.files(d, pattern = "\\.json$", full.names = TRUE)
    }))
    
    total_files <- length(all_files)
    p(sprintf("Total files to process: %d", total_files))
    
    # Le reste du code reste identique
    results <- list()
    for(i in seq(1, total_files, by = batch_size)) {
      batch_end <- min(i + batch_size - 1, total_files)
      current_batch <- all_files[i:batch_end]
      
      batch_results <- future_map(current_batch, 
                                ~process_single_json(.x, id_converter),
                                .progress = FALSE)
      
      batch_results <- batch_results[!sapply(batch_results, is.null)]
      results <- c(results, batch_results)
      
      p(sprintf("Processed %d/%d files", batch_end, total_files))
    }
    
    final_results <- setNames(
      map(results, ~.x$predictions),
      map_chr(results, ~.x$obs_id)
    )
    
    write_json(final_results, output_path, auto_unbox = TRUE)
    
    final_results
  })
}

# Pour tester sur les deux premiers dossiers :
results_test <- process_plant_data(
  base_dir = "00",
  converter_path = "tasks.json",
  output_path = "output_test.json",  # fichier de sortie différent pour le test
  batch_size = 1000
)

# Afficher quelques statistiques sur les résultats du test
cat("Nombre total d'observations traitées:", length(results_test), "\n")
if (length(results_test) > 0) {
  cat("Exemple de la première observation:\n")
  print(results_test[[1]])
}

```
Code qui fonctionne mais qui n'affiche pas les ID

```{r}

# Charger les bibliothèques nécessaires
library(jsonlite)
library(dplyr)
library(purrr)

# Fonction pour traiter un fichier JSON individuel
process_single_json <- function(file_path) {
  # Lire le fichier JSON
  json_data <- tryCatch(
    fromJSON(file_path),
    error = function(e) {
      warning(paste("Erreur lors de la lecture du fichier:", file_path))
      return(NULL)
    }
  )
  
  if (is.null(json_data) || is.null(json_data$results) || length(json_data$results) == 0) return(NULL)
  
  # Extraire l'ID de l'observation depuis le nom du fichier
  obs_id <- tools::file_path_sans_ext(basename(file_path))
  
  # Extraire les prédictions et scores
  results <- json_data$results
  
  # Construire une liste nommée avec les numéros d'espèce comme clés
  predictions <- setNames(results$score, as.character(results$id))
  
  return(list(
    obs_id = obs_id,
    predictions = predictions
  ))
}

# Fonction principale
process_plant_data <- function(base_dir = "00", 
                               output_path = "output_test.json", 
                               max_files = 2) {
  
  # Lister tous les sous-dossiers dans le dossier 00
  subdirs <- list.files(base_dir, full.names = TRUE)
  
  # Initialiser une liste vide pour stocker les résultats
  all_results <- list()
  files_processed <- 0  # Compteur pour limiter le nombre total de fichiers traités
  
  # Pour chaque sous-dossier
  for (subdir in subdirs) {
    # Lister tous les fichiers JSON dans le sous-dossier
    json_files <- list.files(subdir, pattern = "\\.json$", full.names = TRUE)
    
    # Ne traiter que les premiers fichiers jusqu'à atteindre max_files
    json_files <- head(json_files, max_files - files_processed)
    
    # Traiter chaque fichier JSON
    results <- map(json_files, process_single_json)
    
    # Filtrer les résultats NULL
    results <- results[!sapply(results, is.null)]
    
    # Ajouter à nos résultats globaux
    if (length(results) > 0) {
      all_results <- c(all_results, results)
    }
    
    # Mettre à jour le compteur
    files_processed <- files_processed + length(json_files)
    
    # Afficher la progression
    cat(sprintf("Traitement du dossier %s terminé (%d fichiers traités)\n", 
                basename(subdir), length(json_files)))
    
    # Arrêter si on a atteint le nombre maximal de fichiers à traiter
    if (files_processed >= max_files) break
  }
  
  # Vérifier si des résultats ont été collectés
  if (length(all_results) == 0) {
    cat("Aucun fichier JSON valide n'a été traité.\n")
    return(NULL)
  }
  
  # Créer la structure finale
  final_results <- setNames(
    map(all_results, ~.x$predictions),
    map_chr(all_results, ~.x$obs_id)
  )
  
  # Sauvegarder le résultat
  write_json(final_results, output_path, auto_unbox = TRUE, pretty = TRUE)
  
  return(final_results)
}

# Tester avec un petit échantillon
results <- process_plant_data(
  base_dir = "00",              
  output_path = "output_test.json",
  max_files = 2
)

# Vérifier les résultats
if (!is.null(results)) {
  cat("Nombre total d'observations traitées:", length(results), "\n")
  cat("Exemple de la première observation:\n")
  print(results[[1]])
}

```
Test actuel 14h - okk

```{r}

# Fonction pour traiter un fichier JSON individuel
process_single_json <- function(file_path) {
  # Lire le fichier JSON
  json_data <- tryCatch(
    fromJSON(file_path),
    error = function(e) {
      warning(paste("Erreur lors de la lecture du fichier:", file_path))
      return(NULL)
    }
  )
  
  if (is.null(json_data) || is.null(json_data$results) || length(json_data$results) == 0) return(NULL)
  
  # Extraire l'ID de l'observation depuis le nom du fichier
  obs_id <- tools::file_path_sans_ext(basename(file_path))
  
  # Extraire les prédictions et scores sous la forme id: score
  results <- json_data$results
  predictions <- setNames(results$score, as.character(results$id))
  
  # Convertir les résultats pour que l'id soit visible dans la structure finale
  formatted_predictions <- paste(names(predictions), predictions, sep=": ")
  
  return(list(
    obs_id = obs_id,
    predictions = formatted_predictions
  ))
}

# Fonction principale
process_plant_data <- function(base_dir = "00", 
                               output_path = "output_test.json", 
                               max_files = 637) {
  
  # Lister tous les sous-dossiers dans le dossier 00
  subdirs <- list.files(base_dir, full.names = TRUE)
  
  # Initialiser une liste vide pour stocker les résultats
  all_results <- list()
  files_processed <- 0  # Compteur pour limiter le nombre total de fichiers traités
  
  # Pour chaque sous-dossier
  for (subdir in subdirs) {
    # Lister tous les fichiers JSON dans le sous-dossier
    json_files <- list.files(subdir, pattern = "\\.json$", full.names = TRUE)
    
    # Ne traiter que les premiers fichiers jusqu'à atteindre max_files
    json_files <- head(json_files, max_files - files_processed)
    
    # Traiter chaque fichier JSON
    results <- map(json_files, process_single_json)
    
    # Filtrer les résultats NULL
    results <- results[!sapply(results, is.null)]
    
    # Ajouter à nos résultats globaux
    if (length(results) > 0) {
      all_results <- c(all_results, results)
    }
    
    # Mettre à jour le compteur
    files_processed <- files_processed + length(json_files)
    
    # Afficher la progression
    cat(sprintf("Traitement du dossier %s terminé (%d fichiers traités)\n", 
                basename(subdir), length(json_files)))
    
    # Arrêter si on a atteint le nombre maximal de fichiers à traiter
    if (files_processed >= max_files) break
  }
  
  # Vérifier si des résultats ont été collectés
  if (length(all_results) == 0) {
    cat("Aucun fichier JSON valide n'a été traité.\n")
    return(NULL)
  }
  
  # Créer la structure finale : { "nom_fichier" : [id1: score1, id2: score2, ...] }
  final_results <- setNames(
    map(all_results, ~.x$predictions),
    map_chr(all_results, ~.x$obs_id)
  )
  
  # Sauvegarder le résultat sous la forme JSON
  write_json(final_results, output_path, auto_unbox = TRUE, pretty = TRUE)
  
  return(final_results)
}

# Tester avec un petit échantillon
results <- process_plant_data(
  base_dir = "00",              
  output_path = "output_test.json",
  max_files = 637
)

# Vérifier les résultats
if (!is.null(results)) {
  cat("Nombre total d'observations traitées:", length(results), "\n")
  cat("Exemple de la première observation:\n")
  print(results[[1]])
}

```

```{r}

# Fonction pour traiter un fichier JSON individuel
process_single_json <- function(file_path) {
  # Lire le fichier JSON
  json_data <- tryCatch(
    fromJSON(file_path),
    error = function(e) {
      warning(paste("Erreur lors de la lecture du fichier:", file_path))
      return(NULL)
    }
  )
  
  if (is.null(json_data) || is.null(json_data$results) || length(json_data$results) == 0) return(NULL)
  
  # Extraire l'ID de l'observation depuis le nom du fichier
  obs_id <- tools::file_path_sans_ext(basename(file_path))
  
  # Extraire les prédictions et scores sous la forme id: score
  results <- json_data$results
  predictions <- setNames(results$score, as.character(results$id))
  
  # Convertir les résultats pour que l'id soit visible dans la structure finale
  formatted_predictions <- paste(names(predictions), predictions, sep=": ")
  
  return(list(
    obs_id = obs_id,
    predictions = formatted_predictions
  ))
}

# Fonction principale
process_plant_data <- function(base_dir = "00", 
                               output_path = "output_test.json") {
  
  # Lister tous les sous-dossiers dans le dossier 00
  subdirs <- list.files(base_dir, full.names = TRUE)
  
  # Initialiser une liste vide pour stocker les résultats
  all_results <- list()
  files_processed <- 0  # Compteur pour limiter le nombre total de fichiers traités
  
  # Pour chaque sous-dossier
  for (subdir in subdirs) {
    # Lister tous les fichiers JSON dans le sous-dossier
    json_files <- list.files(subdir, pattern = "\\.json$", full.names = TRUE)
    
    # Traiter chaque fichier JSON
    results <- map(json_files, process_single_json)
    
    # Filtrer les résultats NULL
    results <- results[!sapply(results, is.null)]
    
    # Ajouter à nos résultats globaux
    if (length(results) > 0) {
      all_results <- c(all_results, results)
    }
    
    # Mettre à jour le compteur
    files_processed <- files_processed + length(json_files)
    
    # Afficher la progression
    cat(sprintf("Traitement du dossier %s terminé (%d fichiers traités)\n", 
                basename(subdir), length(json_files)))
    
  }
  
  # Vérifier si des résultats ont été collectés
  if (length(all_results) == 0) {
    cat("Aucun fichier JSON valide n'a été traité.\n")
    return(NULL)
  }
  
  # Créer la structure finale : { "nom_fichier" : [id1: score1, id2: score2, ...] }
  final_results <- setNames(
    map(all_results, ~.x$predictions),
    map_chr(all_results, ~.x$obs_id)
  )
  
  # Sauvegarder le résultat sous la forme JSON
  write_json(final_results, output_path, auto_unbox = TRUE, pretty = TRUE)
  
  return(final_results)
}

# Tester avec un petit échantillon
results <- process_plant_data(
  base_dir = "00",              
  output_path = "output_test.json"
)

# Vérifier les résultats
if (!is.null(results)) {
  cat("Nombre total d'observations traitées:", length(results), "\n")
  cat("Exemple de la première observation:\n")
  print(results[[1]])
}

```

```















